# Simple Link Scraper ğŸ•·ï¸

A Rust-based web crawler that creates a graph visualization of linked web pages, starting from a given URL.

## âœ¨ Features

- ğŸ” Crawls web pages starting from rust-lang.org
- ğŸ“Š Creates a directed graph of webpage connections
- ğŸ›‘ Limits crawling to 1000 URLs to prevent excessive requests
- ğŸ“„ Generates a DOT file for graph visualization
- âš¡ Asynchronous HTTP requests using tokio and reqwest
- ğŸ” HTML parsing with scraper crate

## ğŸ“¦ Dependencies

- `petgraph`: Graph data structure and algorithms
- `reqwest`: HTTP client
- `scraper`: HTML parsing
- `tokio`: Async runtime
- `url`: URL parsing

## ğŸš€ Usage

1. Clone the repository
2. Build the project:

```sh
cargo build
```

3. Run the crawler:

```sh
cargo run
```

4. The program will:
   - Start crawling from https://www.rust-lang.org
   - Print visited URLs and new links found
   - Generate a `graph.dot` file when finished

## ğŸ¨ Visualization

To visualize the generated graph, install Graphviz and run:

```sh
dot -Tpng graph.dot > graph.png
```

## âš ï¸ Note

The crawler is rate-limited to 1000 URLs to prevent excessive requests to web servers. Adjust the limit in the code if needed.

## ğŸ“ License

MIT
